{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding (sinusoid)"
      ],
      "metadata": {
        "id": "ZOinJbTapJVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk6rKmPkM6Jn"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # 주어진 공식대로 적용해주면 된다.\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3,..\n",
        "        # 10000^(2i/dim_model) 짝수로 .. positions list를 나눠줄건데 sin cos 번갈아가면서 가기 때문인 것 같다.\n",
        "        division_term = torch.exp(torch.arange(0, dim_model,2).float() * (-math.log(10000.0))/dim_model)\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "\n",
        "        # Register buffer 로 저장하면 훈련없이 한 레이어로 적용한다.\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1) # (1, max_len, dim_model) -> (max_len, 1, dim_model)\n",
        "        # 위의 transpose는 batch_first = False 인 경우 elementwise operation 하나부다.\n",
        "\n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
        "\n",
        "# 이런식으로 작동하는구나~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymFLbqfPQtpx"
      },
      "source": [
        "# BERT 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordPiece Tokenizer, BPE (Byte Pair Encoding)"
      ],
      "metadata": {
        "id": "mukf9JphoEum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "INkwCMZhSkTK",
        "outputId": "26cde87a-db1d-41b6-bf7f-fd06f548fd09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Jet makers feud over seat width with big orders at stake'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# 하기 전에 우선 WordPiece Tokenizer 부터 공부하자.\n",
        "\n",
        "# Word Piece -> 단어를 표현할 수 있는 subwords units로 모든 단어를 표현 /\n",
        "# RNN과 같은 word embedding vectors를 사용 -> 단어 개수만큼 embedding vector를 학습. ㅜㅜ 너무 많아\n",
        "# -> 제한된 개수의 단어를 사용하자. but long tail 이론에 의거하여 조그마한 단어를 무시함녀 미등록단어 문제 발생.\n",
        "\n",
        "# 언어는 글자를 subword units으로 사용. (알파벳..) 하지만 이러한 유닛은 개념을 지칭하기는 어렵다.\n",
        "# 영어에 있어서는 이러한 unit은 모호성이 너무 강하다. --> 잘 나눠서 unit으로 사용하자.\n",
        "\n",
        "# WPM\n",
        "# Jet makers feud over seat width with big orders at stake.\n",
        "# Wordpieces : _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake\n",
        "# _ -> 문장 복원을 위함.. 시작 단어를 _로 분할한다. Jet이나 feud 와 같은 단어는 자주 등장하지 않아서 띄어쓰기로 또 분할된다.\n",
        "\n",
        "# recover function\n",
        "def recover(tokens):\n",
        "    sent = \"\".join(tokens)\n",
        "    sent = sent.replace('_', \" \")\n",
        "    return sent\n",
        "tokens = [\"_J\", \"et\", \"_makers\", \"_fe\", \"ud\", \"_over\", \"_seat\", \"_width\", \"_with\", \"_big\", \"_orders\", \"_at\", \"_stake\"]\n",
        "recover(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmd3KSIMUjAz",
        "outputId": "ac788e91-6af2-4977-bd77-bdb217c280bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('e', 's')\n",
            "('es', 't')\n",
            "('est', '</w>')\n",
            "('l', 'o')\n",
            "('lo', 'w')\n",
            "('n', 'e')\n",
            "('ne', 'w')\n",
            "('new', 'est</w>')\n",
            "('low', '</w>')\n",
            "('w', 'i')\n",
            "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
          ]
        }
      ],
      "source": [
        "# BPE. Byte-pair Encoding\n",
        "# for 문으로 가장 빈도수가 많은 bigram을 찾는다. character가 units 이다.\n",
        "\n",
        "# 배경. 어떤 document에서 단어의 빈도를 구한다. -> 각 단어의 character 사이의 공백 넣고 마지막엔 </w>\n",
        "\n",
        "# 이 데이터에 대해 stats를 받고 merge 한다.\n",
        "\n",
        "\n",
        "\n",
        "import re, collections\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq # 우선 bigram 단위로 frequency 확인..\n",
        "            # merge 후 계속 적용한다.\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair)) # 특수문자를 \\로 바꿔준다. (escape 한다라고 함.)\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "vocab = {'l o w </w>' : 5,\n",
        "         'l o w e r </w>' : 2,\n",
        "         'n e w e s t </w>':6,\n",
        "         'w i d e s t </w>':3\n",
        "         }\n",
        "\n",
        "num_merges = 10 # 10 번 merge 진행할거임\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    best = max(pairs, key=pairs.get) # 현재 vocab 가장 좋은 bigram\n",
        "    vocab = merge_vocab(best, vocab) # vocab merge\n",
        "    print(best)\n",
        "\n",
        "print(vocab)\n",
        "# 이런식으로 WPM 에서의 subwords 개념을 만들어낼 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdbSCpLNXxAK",
        "outputId": "001de60c-20c6-4042-c3a5-92852d14e7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ],
      "source": [
        "# 구글에서 발표한 sentencepiece 를 사용할 수 있다.\n",
        "! pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sentencepiece 라이브러리 활용"
      ],
      "metadata": {
        "id": "3WPa76mBoLiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ye7dmMp2ayuo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "path = '/content/drive/MyDrive/Dataset/'\n",
        "# tokenizer 사용.\n",
        "parameter = \"--input={} --model_prefix={} --vocab_size={} --user_defined_symbols={} --model_type={} --character_coverage={}\"\n",
        "\n",
        "# cmd 형태로 주는데?\n",
        "\n",
        "train_input_file = path + \"BookCorpus_train.txt\"\n",
        "vocab_size = 30000\n",
        "prefix = 'bookcorpus_spm'\n",
        "user_defined_symbols = \"[PAD],[CLS],[SEP],[MASK]\"\n",
        "model_type = \"bpe\" # Byte Pair Encoding (Bigram..)\n",
        "character_coverage = 1.0 # default\n",
        "\n",
        "cmd = parameter.format(train_input_file, prefix, vocab_size, user_defined_symbols, model_type, character_coverage)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(cmd) # 주어진 corpus로 tokenizer를 훈련시킨다. heuristic 한 방법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset!"
      ],
      "metadata": {
        "id": "dfVxmRRWpS71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8_lCgSGPdIzM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# Dataset 만드는건 torchtext dataset을 사용할 수 있지만 곧 사라진다.\n",
        "# torch.utils.data.Dataset 사용해서 커스텀 데이터셋 만들자.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    BPE(Byte Pair Encoding) 방법으로 pretrained vocab을 가져와서 list[str] 형 데이터에 적용시켜줄거임.\n",
        "\n",
        "    특정 토큰으로\n",
        "    MLM 을 위한 [MASK]\n",
        "    NSP 에서 다음 문장에 대한 [SEP]\n",
        "    sequence length를 맞춰주기위한 패딩 [PAD]\n",
        "    Classification Token [CLS]\n",
        "\"\"\"\n",
        "\n",
        "class LanguageModelingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, vocab: spm.SentencePieceProcessor, sep_id: str=\"[SEP]\", cls_id: str='[CLS]',\n",
        "                 mask_id: str='[MASK]', pad_id: str='[PAD]', seq_len: int=512, mask_frac: float=0.15, p: float=0.5):\n",
        "        \n",
        "        # vocab으로 BPE 적용 \n",
        "        super(LanguageModelingDataset, self).__init__()\n",
        "        self.vocab = vocab # SentencePieceProcessor\n",
        "        self.data = data # language\n",
        "        self.seq_len = seq_len\n",
        "        self.sep_id = vocab.piece_to_id(sep_id) # 각 특수 토큰에 대응하는 id 가져오기\n",
        "        self.cls_id = vocab.piece_to_id(cls_id)\n",
        "        self.mask_id = vocab.piece_to_id(mask_id)\n",
        "        self.pad_id = vocab.piece_to_id(pad_id)\n",
        "        self.p = p\n",
        "        self.mask_frac = mask_frac\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        seq1 = self.vocab.EncodeAsIds(self.data[i].strip()) # 우선 data 하나 가져온다. sequence 단위\n",
        "        seq2_idx = i+1\n",
        "        if random.random() < self.p : # 다음 sequence가 아닌가?.. 확률로 배정\n",
        "            is_next = torch.tensor(0)\n",
        "            while seq2_idx != i+1:\n",
        "                seq2_idx = random.randint(0, len(self.data))\n",
        "        else :\n",
        "            is_next = torch.tensor(0)\n",
        "\n",
        "        seq2 = self.vocab.EncodeAsIds(self.data[seq2_idx])\n",
        "\n",
        "        if len(seq1) + len(seq2) >= self.seq_len - 3:\n",
        "            idx = self.seq_len - 3 - len(seq1)\n",
        "            seq2 = seq2[:idx]  # 크기 제한\n",
        "\n",
        "        # mask가 적용이 안된 상태의 seq.. -> 사전에 정해둔 sequence length를 맞춰주기 위해서 패딩을 지정해준다.\n",
        "        mlm_target = torch.tensor([self.cls_id] + seq1 + [self.sep_id] + seq2 + [self.sep_id] + [self.pad_id] * (self.seq_len -3 - len(seq1) - len(seq2))).long().contiguous()\n",
        "        sent_emb = torch.ones((mlm_target.size(0)))\n",
        "        _idx = len(seq1) + 2\n",
        "\n",
        "        # sentence embedding : 0 -> A, 1 -> B\n",
        "        sent_emb[:_idx] = 0\n",
        "\n",
        "        def masking(data):\n",
        "            data = torch.tensor(data).long().contiguous()\n",
        "            data_len = data.size(0)\n",
        "            ones_num = int(data_len * self.mask_frac) # masking 갯수\n",
        "            zeros_num = data_len - ones_num # non masked\n",
        "            lm_mask = torch.cat([torch.zeros(zeros_num), torch.ones(ones_num)])\n",
        "            lm_mask = lm_mask[torch.randperm(data_len)]  # data length 만큼 arange 이후 셔플.. 결국 lm mask 셔플한거임.\n",
        "            data = data.masked_fill(lm_mask.bool(), self.mask_id) # data에 lm_mask가 True인 경우 (mask 적용하는 경우) \n",
        "            # mask_id 로 채워버린다.\n",
        "\n",
        "            return data\n",
        "        # token은 놔두고 각 sequence에 대해 마스킹 적용 \n",
        "        mlm_train = torch.cat([torch.tensor([self.cls_id]), masking(seq1), torch.tensor([self.sep_id]), masking(seq1), torch.tensor([self.sep_id])]).long().contiguous()\n",
        "        # 크기 맞춰주기 위해 패딩 넣어주기.\n",
        "        mlm_train = torch.cat([mlm_train, torch.tensor([self.pad_id] * (512 - mlm_train.size(0)))]).long().contiguous()\n",
        "\n",
        "        return mlm_train, mlm_target, sent_emb, is_next\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) -1 # NSP에서 마지막 문장은 안된다. -> 마지막 문장에서 -1 idx에 대해서만 사용 가능. DataLoader에서 getitem 접근할 때 idx 부여를 len(data)를 기반으로 적용하기 때문에 이렇게 놔두자.\n",
        "    \n",
        "    def __iter__(self):\n",
        "        for x in self.data:\n",
        "            yield x\n",
        "    \n",
        "    def get_vocab(self):\n",
        "        return self.vocab\n",
        "    \n",
        "    def decode(self, x):\n",
        "        return self.vocab.DecodeIds(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실행되는지만 확인하려고 BookCorpus data를 쪼개서 가져왔음"
      ],
      "metadata": {
        "id": "kXR2Lp2-pcHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "413IpBGadnPN"
      },
      "outputs": [],
      "source": [
        "file = \"/content/drive/MyDrive/Dataset/BookCorpus_train.txt\"\n",
        "data = []\n",
        "data_len = 8000 # Colab RAM 제한 문제. 일부만 가져오자!\n",
        "with open(file) as f: \n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        if i >= data_len:\n",
        "            break\n",
        "        data.append(line.strip()) # txt 파일 line 읽기\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = \"bookcorpus_spm.model\" # pretrained vocab (BPE) 위에서 spm train 시키면 spm.model 파일 생긴다.\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "dataset = LanguageModelingDataset(data=data, vocab=vocab)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "zDz7IwoWVBQy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (mlm_train, mlm_target, sent_emb, is_next) in enumerate(dataloader):\n",
        "    print(mlm_train.size())\n",
        "    print(mlm_target.size())\n",
        "    print(sent_emb.size())\n",
        "    print(is_next.size())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJAGqvgpU2bN",
        "outputId": "687c9bf7-67f2-4678-8a47-f7f3a1443afb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 512])\n",
            "torch.Size([32, 512])\n",
            "torch.Size([32, 512])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert Model"
      ],
      "metadata": {
        "id": "ttEeVOGopjIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    주의할점 _ Batch First 상태이다. 데이터를 그렇게 불러오는중.\n",
        "\"\"\"\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k, pad_index):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(pad_index).unsqueeze(1)\n",
        "    pad_attn_mask = torch.as_tensor(pad_attn_mask, dtype=torch.float)\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k) # batch first\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model=768, seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.emb = nn.Embedding(seq_len, d_model) # 위치에 따른 고유값.\n",
        "\n",
        "    def forward(self, x):\n",
        "        pos = torch.arange(self.seq_len, dtype=torch.long, device=x.device)\n",
        "        pos = pos.unsqueeze(0).expand(x.size())\n",
        "        pos_emb = self.emb(pos)\n",
        "        return self.dropout(pos_emb)\n",
        "\n",
        "\n",
        "class BERTEmbedding(nn.Module):\n",
        "    def __init__(self, seq_len=512, voc_size=30000, d_model=768, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(num_embeddings=voc_size, embedding_dim=d_model) # 30000개 단어 포함하였기 때문 \n",
        "        # 30000개의 고유값을 768에 욱여넣기. B, S, d_model shape\n",
        "        self.tok_dropout = nn.Dropout(dropout)\n",
        "        self.seg_emb = nn.Embedding(num_embeddings=2, embedding_dim=d_model)\n",
        "        self.seg_dropout = nn.Dropout(dropout)\n",
        "        self.pos_emb = PositionalEncoding(d_model, seq_len, dropout)\n",
        "\n",
        "\n",
        "    def forward(self, data, seg_emb):\n",
        "        tok_emb = self.tok_emb(data) #  B, S, d_model shape\n",
        "        seg_emb = self.seg_emb(seg_emb) #  B, S, d_model shape\n",
        "        pos_emb = self.pos_emb(data) #  B, S, d_model shape \n",
        "\n",
        "        return self.tok_dropout(tok_emb) + self.seg_dropout(seg_emb) + pos_emb\n",
        "\n",
        "\n",
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, voc_size=30000, seq_len=512, d_model=768, d_ff=3072, pad_idx=1, num_encoder=12,\n",
        "                 num_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.emb = BERTEmbedding(seq_len, voc_size, d_model, dropout)\n",
        "        encoder = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, dropout=dropout, batch_first=True)\n",
        "        self.encoders = nn.TransformerEncoder(encoder, num_layers=num_encoder)\n",
        "\n",
        "        self.mlm = nn.Linear(d_model, voc_size) # MLM Task classification 처럼 ..\n",
        "        self.nsp = nn.Linear(d_model, 2) # NSP Task classificiation 처럼..\n",
        "        \n",
        "\n",
        "    def forward(self, input, seg):\n",
        "        pad_mask = get_attn_pad_mask(input, input, self.pad_idx) \n",
        "\n",
        "        emb = self.emb(input, seg) # ( B, S, d_model ) shape\n",
        "\n",
        "        # mask는 batch size 없이 S, S 형태로만 들어가야한다.\n",
        "        feat = self.encoders(emb, pad_mask[0])# ( B, S, d_model ) shape\n",
        "    \n",
        "        \n",
        "        mlm = self.mlm(feat) # B, S, D_model\n",
        "        nsp = self.nsp(feat)\n",
        "        return mlm, nsp[:,0,:] # NSP : (B, 2), Class token만 사용하기 때문.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pebonlo9U2VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실행은 대략 이런식으로 ..\n",
        "\n",
        "모델이 너무 커서 코랩 무료 버전으로 돌아가질 않는다.  \n",
        "그래서 대략만 짜고 넘어간다.  \n",
        "data가 어떤 shape을 가지고, 어떤 형태로 흘러들어가는지, 모델을 거치면서 어떤 operation이 적용되는 정도를 파악하기 위해 작성했다."
      ],
      "metadata": {
        "id": "kdgSNUKsppkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BERTModel().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    m_losses = []\n",
        "    n_losses = []\n",
        "    print(f'epoch : {epoch}/100')\n",
        "    for batch, (mlm_train, mlm_target, sent_emb, is_next) in enumerate(dataloader):\n",
        "        sent_emb = sent_emb.long()\n",
        "        mlm_train, mlm_target, sent_emb, is_next = mlm_train.to(device), mlm_target.to(device), sent_emb.to(device), is_next.to(device)\n",
        "\n",
        "        mlm, nsp = model(mlm_train, sent_emb) # 허허..\n",
        "\n",
        "        m_loss = criterion(mlm, mlm_target)\n",
        "        n_loss = criterion(nsp, is_next)\n",
        "\n",
        "        m_losses.append(m_loss.item())\n",
        "        n_losses.append(n_loss.item())\n",
        "\n",
        "        loss = m_loss + n_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"MLM : {np.mean(m_losses)}, NSP : {np.mean(n_losses)}\") # 대충 이런식으로 pretrain이 진행된다!\n",
        "\n"
      ],
      "metadata": {
        "id": "MSPPE4-9hOGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LA6qtL_4iWZl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch_BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}